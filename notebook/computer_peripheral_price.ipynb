{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the link is : https://modxcomputers.com/product-category/pc-components/processor/, and no of pages are : 5\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/motherboard/, and no of pages are : 13\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/cpu-cooler/, and no of pages are : 9\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/ram/, and no of pages are : 4\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/graphics-card/, and no of pages are : 6\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/ssd/, and no of pages are : 4\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/hard-drive/, and no of pages are : 0\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/power-supply/, and no of pages are : 5\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/cabinet/, and no of pages are : 13\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/case-fans/, and no of pages are : 2\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/custom-cables/, and no of pages are : 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as ps\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime # add this in the code and make a column that will store the date and time of the data scraping \n",
    "\n",
    "url = 'https://modxcomputers.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "pcComponents_url_list = []\n",
    "for hrefs in soup.find_all('a', href=True):\n",
    "    # print(hrefs['href'])\n",
    "    if \"https://modxcomputers.com/product-category/pc-components/\" in hrefs['href']:\n",
    "        url = urljoin(url, hrefs['href'])\n",
    "        if len(urlparse(url).path.split('/')) <= 5:\n",
    "            pcComponents_url_list.append(hrefs['href'])\n",
    "\n",
    "pcComponents_url_lists = list(dict.fromkeys(pcComponents_url_list))\n",
    "\n",
    "product_url_list = []\n",
    "product_name_list = []\n",
    "product_rating_list = []\n",
    "product_price_list = []\n",
    "product_in_stock_list = []\n",
    "product_scraped_date_time = []\n",
    "\n",
    "for link in pcComponents_url_lists:\n",
    "    try:\n",
    "        time.sleep(0.5)\n",
    "        product_page = requests.get(link, headers=headers)\n",
    "        product_soup = BeautifulSoup(product_page.content, 'html.parser')\n",
    "        # this is to get the product details for products with multiple pages.\n",
    "        if product_soup.find('div',{\"class\":\"wd-loop-footer products-footer\"}):\n",
    "            pages = product_soup.find('div',{\"class\":\"wd-loop-footer products-footer\"}).find('ul',{\"class\":\"page-numbers\"}).find_all('li')[-2].text\n",
    "            print(f\"the link is : {link}, and no of pages are : {pages}\")\n",
    "            for page in range(1, int(pages)+1):\n",
    "                time.sleep(1)\n",
    "                inner_page_url = f\"{link}page/{page}/\"\n",
    "                # print(inner_page_url)\n",
    "                inner_page = requests.get(inner_page_url, headers=headers)\n",
    "                inner_soup = BeautifulSoup(inner_page.content, 'html.parser')\n",
    "                inner_products_blocks = inner_soup.find_all('div',{\"class\":\"product-wrapper\"})\n",
    "                for product in inner_products_blocks:\n",
    "\n",
    "                    product_url = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').find('a')['href']\n",
    "                    if product_url:\n",
    "                        product_url_list.append(product_url)\n",
    "                    else:\n",
    "                        product_url_list.append('Not Found')\n",
    "                    \n",
    "                    product_name = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').text\n",
    "                    if product_name:\n",
    "                        product_name_list.append(product_name)\n",
    "                    else:\n",
    "                        product_name_list.append('Not Found')\n",
    "                    \n",
    "                    product_rating = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wd-star-rating\"}).text.strip().split(' ')[1]\n",
    "                    if product_rating:\n",
    "                        product_rating_list.append(product_rating)\n",
    "                    else:\n",
    "                        product_rating_list.append('Not Found')\n",
    "                    \n",
    "                    product_price = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wrap-price\"}).text.strip()\n",
    "                    if product_price:\n",
    "                        product_price_list.append(product_price)\n",
    "                    else:\n",
    "                        product_price_list.append('Not Found')\n",
    "                    \n",
    "                    product_in_stock = product.find('div',{\"class\":\"product-element-bottom\"}).find('p').text.strip()\n",
    "                    if product_in_stock:\n",
    "                        product_in_stock_list.append(product_in_stock)\n",
    "                    else:\n",
    "                        product_in_stock_list.append('Not Found')\n",
    "                    \n",
    "                    now_time = datetime.now().strftime('%d/%m/%Y-%H:%M')\n",
    "                    product_scraped_date_time.append(now_time)\n",
    "\n",
    "        # this is to get the product details for products with one page.\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "            print(f\"the link is : {link}, and no of pages are : 0\")\n",
    "            inner_page = requests.get(link, headers=headers)\n",
    "            inner_soup = BeautifulSoup(inner_page.content, 'html.parser')\n",
    "            inner_products_blocks = inner_soup.find_all('div',{\"class\":\"product-wrapper\"})\n",
    "\n",
    "            for product in inner_products_blocks:\n",
    "                \n",
    "                product_url = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').find('a')['href']\n",
    "                if product_url:\n",
    "                    product_url_list.append(product_url)\n",
    "                else:\n",
    "                    product_url_list.append('Not Found')\n",
    "                \n",
    "                product_name = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').text\n",
    "                if product_name:\n",
    "                    product_name_list.append(product_name)\n",
    "                else:\n",
    "                    product_name_list.append('Not Found')\n",
    "                \n",
    "                product_rating = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wd-star-rating\"}).text.strip().split(' ')[1]\n",
    "                if product_rating:\n",
    "                    product_rating_list.append(product_rating)\n",
    "                else:\n",
    "                    product_rating_list.append('Not Found')\n",
    "                \n",
    "                product_price = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wrap-price\"}).text.strip()\n",
    "                if product_price:\n",
    "                    product_price_list.append(product_price)\n",
    "                else:\n",
    "                    product_price_list.append('Not Found')\n",
    "                \n",
    "                product_in_stock = product.find('div',{\"class\":\"product-element-bottom\"}).find('p').text.strip()\n",
    "                if product_in_stock:\n",
    "                    product_in_stock_list.append(product_in_stock)\n",
    "                else:\n",
    "                    product_in_stock_list.append('Not Found')\n",
    "                \n",
    "                now_time = datetime.now().strftime('%d/%m/%Y-%H:%M')\n",
    "                product_scraped_date_time.append(now_time)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise(f'Exception occurred : {e}')\n",
    "\n",
    "data = {\n",
    "    'product_scraped_date_time' : product_scraped_date_time,\n",
    "    'product_name' : product_name_list,\n",
    "    'product_rating_of_5' : product_rating_list,\n",
    "    'product_price' : product_price_list,\n",
    "    'product_in_stock' : product_in_stock_list,\n",
    "    'product_url' : product_url_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the link is : https://modxcomputers.com/product-category/pc-components/processor/, and no of pages are : 5\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/motherboard/, and no of pages are : 13\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/cpu-cooler/, and no of pages are : 9\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/ram/, and no of pages are : 4\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/graphics-card/, and no of pages are : 6\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/ssd/, and no of pages are : 4\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/hard-drive/, and no of pages are : 0\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/power-supply/, and no of pages are : 5\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/cabinet/, and no of pages are : 13\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/case-fans/, and no of pages are : 2\n",
      "the link is : https://modxcomputers.com/product-category/pc-components/custom-cables/, and no of pages are : 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, base_url, user_agent, headers=None):\n",
    "        self.base_url = base_url\n",
    "        self.user_agent = user_agent\n",
    "        self.headers = headers\n",
    "        self.pcComponents_url_list = []\n",
    "        self.product_data = []\n",
    "        self.product_url_list = []\n",
    "        self.product_name_list = []\n",
    "        self.product_rating_list = []\n",
    "        self.product_price_list = []\n",
    "        self.product_in_stock_list = []\n",
    "        self.product_scraped_date_time = []\n",
    "\n",
    "    def _get_soup(self, url):\n",
    "        page = requests.get(url, headers=self.headers)\n",
    "        return BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    def scrape(self):\n",
    "        soup = self._get_soup(self.base_url)\n",
    "        for hrefs in soup.find_all('a', href=True):\n",
    "            if \"https://modxcomputers.com/product-category/pc-components/\" in hrefs['href']:\n",
    "                url = urljoin(base_url, hrefs['href'])\n",
    "                if len(urlparse(url).path.split('/')) <= 5:\n",
    "                    self.pcComponents_url_list.append(hrefs['href'])\n",
    "\n",
    "        self.pcComponents_url_list = list(dict.fromkeys(self.pcComponents_url_list))\n",
    "        \n",
    "        for link in self.pcComponents_url_list:\n",
    "            try:\n",
    "                time.sleep(0.5)\n",
    "                product_page = requests.get(link, headers=self.headers)\n",
    "                product_soup = BeautifulSoup(product_page.content, 'html.parser')\n",
    "                # this is to get the product details for products with multiple pages.\n",
    "                if product_soup.find('div',{\"class\":\"wd-loop-footer products-footer\"}):\n",
    "                    pages = product_soup.find('div',{\"class\":\"wd-loop-footer products-footer\"}).find('ul',{\"class\":\"page-numbers\"}).find_all('li')[-2].text\n",
    "                    print(f\"the link is : {link}, and no of pages are : {pages}\")\n",
    "                    for page in range(1, int(pages)+1):\n",
    "                        time.sleep(1)\n",
    "                        inner_page_url = f\"{link}page/{page}/\"\n",
    "                        # print(inner_page_url)\n",
    "                        inner_page = requests.get(inner_page_url, headers=self.headers)\n",
    "                        inner_soup = BeautifulSoup(inner_page.content, 'html.parser')\n",
    "                        inner_products_blocks = inner_soup.find_all('div',{\"class\":\"product-wrapper\"})\n",
    "                        for product in inner_products_blocks:\n",
    "\n",
    "                            product_url = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').find('a')['href']\n",
    "                            if product_url:\n",
    "                                self.product_url_list.append(product_url)\n",
    "                            else:\n",
    "                                self.product_url_list.append('Not Found')\n",
    "                            \n",
    "                            product_name = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').text\n",
    "                            if product_name:\n",
    "                                self.product_name_list.append(product_name)\n",
    "                            else:\n",
    "                                self.product_name_list.append('Not Found')\n",
    "                            \n",
    "                            product_rating = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wd-star-rating\"}).text.strip().split(' ')[1]\n",
    "                            if product_rating:\n",
    "                                self.product_rating_list.append(product_rating)\n",
    "                            else:\n",
    "                                self.product_rating_list.append('Not Found')\n",
    "                            \n",
    "                            product_price = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wrap-price\"}).text.strip()\n",
    "                            if product_price:\n",
    "                                self.product_price_list.append(product_price)\n",
    "                            else:\n",
    "                                self.product_price_list.append('Not Found')\n",
    "                            \n",
    "                            product_in_stock = product.find('div',{\"class\":\"product-element-bottom\"}).find('p').text.strip()\n",
    "                            if product_in_stock:\n",
    "                                self.product_in_stock_list.append(product_in_stock)\n",
    "                            else:\n",
    "                                self.product_in_stock_list.append('Not Found')\n",
    "                            \n",
    "                            self.product_scraped_date_time.append(datetime.now().strftime('%d/%m/%Y-%H:%M'))\n",
    "\n",
    "                # this is to get the product details for products with one page.\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "                    print(f\"the link is : {link}, and no of pages are : 0\")\n",
    "                    inner_page = requests.get(link, headers=headers)\n",
    "                    inner_soup = BeautifulSoup(inner_page.content, 'html.parser')\n",
    "                    inner_products_blocks = inner_soup.find_all('div',{\"class\":\"product-wrapper\"})\n",
    "\n",
    "                    for product in inner_products_blocks:\n",
    "                        \n",
    "                        product_url = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').find('a')['href']\n",
    "                        if product_url:\n",
    "                            self.product_url_list.append(product_url)\n",
    "                        else:\n",
    "                            self.product_url_list.append('Not Found')\n",
    "                        \n",
    "                        product_name = product.find('div',{\"class\":\"product-element-bottom\"}).find('h3').text\n",
    "                        if product_name:\n",
    "                            self.product_name_list.append(product_name)\n",
    "                        else:\n",
    "                            self.product_name_list.append('Not Found')\n",
    "                        \n",
    "                        product_rating = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wd-star-rating\"}).text.strip().split(' ')[1]\n",
    "                        if product_rating:\n",
    "                            self.product_rating_list.append(product_rating)\n",
    "                        else:\n",
    "                            self.product_rating_list.append('Not Found')\n",
    "                        \n",
    "                        product_price = product.find('div',{\"class\":\"product-element-bottom\"}).find('div',{\"class\":\"wrap-price\"}).text.strip()\n",
    "                        if product_price:\n",
    "                            self.product_price_list.append(product_price)\n",
    "                        else:\n",
    "                            self.product_price_list.append('Not Found')\n",
    "                        \n",
    "                        product_in_stock = product.find('div',{\"class\":\"product-element-bottom\"}).find('p').text.strip()\n",
    "                        if product_in_stock:\n",
    "                            self.product_in_stock_list.append(product_in_stock)\n",
    "                        else:\n",
    "                            self.product_in_stock_list.append('Not Found')\n",
    "\n",
    "                        self.product_scraped_date_time.append(datetime.now().strftime('%d/%m/%Y-%H:%M'))\n",
    "            except Exception as e:\n",
    "                raise(f'Exception occurred : {e}')\n",
    "\n",
    "            self.product_data = {\n",
    "                    'product_scraped_date_time' : self.product_scraped_date_time,\n",
    "                    'product_name' : self.product_name_list,\n",
    "                    'product_rating_of_5' : self.product_rating_list,\n",
    "                    'product_price' : self.product_price_list,\n",
    "                    'product_in_stock' : self.product_in_stock_list,\n",
    "                    'product_url' : self.product_url_list\n",
    "            }\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        df = pd.DataFrame(self.product_data)\n",
    "        return df\n",
    "\n",
    "    def run(self):\n",
    "        self.scrape()\n",
    "        df = self.to_dataframe()\n",
    "        return df\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    base_url = 'https://modxcomputers.com/'\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    scraper = WebScraper(base_url, user_agent, headers)\n",
    "    df = scraper.run()\n",
    "\n",
    "    df.to_csv('scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
